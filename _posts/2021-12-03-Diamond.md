---
title: "다이아몬드의 특성에 따른 가격 예측"
toc: true
toc_sticky: true
categories:
  - ML/DL
---
# Diamonds Price Prediction

## Members

|학과       |학번       |이름  |Email             |
|----------|----------|----|-------------------|
|소프트웨어학부|2017012397|이재훈|yeolkeut@gmail.com|

## 1. Proposal

### 1.1. Motivation
![diamond](/assets/images/diamond.jpg)
다이아몬드는 희귀한데다 특유의 광택으로 인해 예로부터 귀한 보석으로 여겨졌다. 그러나 질 좋은 다이아몬드는 매우 드물고 그 가치는 다이아몬드가 갖는 여러 특성에 의해 결정된다. 다이아몬드에 대한 데이터셋을 사용해 모델을 학습시키고 이를 통해 다른 다이아몬드의 가격을 예측해 보면 재밌겠다는 생각이 들어 이를 주제로 프로젝트를 진행하기로 했다.

### 1.2. Goal
주어진 데이터셋을 이용하여 모델을 학습시키고 다른 다이아몬드의 특성과 학습시킨 모델을 통해 다른 다이아몬드의 가격을 예측할 수 있다.

## 2. Datasets
[Kaggle](https://www.kaggle.com)에 있는 [Diamonds](https://www.kaggle.com/shivam2503/diamonds) 데이터셋을 사용하기로 했다. 이 데이터셋은 약 54,000개의 다이아몬드의 특성에 대한 데이터를 포함한다.

### 2.1. Load Datasets
```python
import pandas as pd

df = pd.read_csv("./diamonds.csv")
df.head()
```

<img src="../assets/images/result1.png" width="100%">

### 2.2. Feature Details
1. **carat**: 다이아몬드의 무게(단위: 캐럿). (0.2 ~ 5.01)

2. **cut**: 가공의 품질. (Fair, Good, Very Good, Premium, Ideal)

3. **color**: 다이아몬드의 색. (J(최하급) ~ D(최상급))

4. **clarity**: 다이아몬드의 투명도. (I1(최하급), SI2, SI1, VS2, VS1, VVS2, VVS1, IF(최상급))

5. **depth**: 총 깊이 비율 = z / mean(x, y) = 2 * z / (x + y). (43 ~ 79)

6. **table**: 가장 넓은 점을 기준으로 한 다이아몬드 상단의 폭 비율. (43 ~ 95)

7. **price**: 다이아몬드의 가격(단위: 달러) ($326 ~ $18,823)

8. **x**: 길이(단위: mm) (0 ~ 10.74)

9.  **y**: 너비(단위: mm) (0 ~ 58.9)

10. **z**: 깊이(단위: mm) (0 ~ 31.8)

## 3. Data Preprocessing
데이터 전처리는 분석 결과와 모델 성능에 직접적인 영향을 미치는 과정이기 때문에 매우 중요하다.
이번 프로젝트에서 데이터 전처리를 위해 다음과 같은 과정을 진행한다.
- Data Cleaning
- Outlier Identify & Remove
- Feature Encode

### 3.1. Data Cleaning
```python
df.drop('Unnamed: 0', axis=1, inplace=True)

df = df[df['x'] != 0]
df = df[df['y'] != 0]
df = df[df['z'] != 0]
```

"Unnamed: 0" Column은 유효하지 않은 Column이므로 Drop한다.

또한, 다이아몬드는 3차원이기 때문에 x, y, z 중 어느 값 하나라도 0이되면 유효하지 않으므로 해당 Row를 제거한다.

```python
df.info()
df.isnull().sum()
```

<img src="../assets/images/result2.png" width="50%">

<img src="../assets/images/result3.png" width="20%">

유효하지 않은 Data를 제거해 Data Cleaning을 수행했다.

### 3.2. Outlier Identify & Remove
```python
df.describe()
```

<img src="../assets/images/result4.png" width="100%">

대상 변수는 Price이므로 Price와 다른 변수와의 관계를 확인해보자.

```python
import seaborn as sns

columns_list = df.select_dtypes(exclude='object').columns.to_list()
columns_list.remove('price')

fig = plt.figure(figsize=(15, 10), constrained_layout=True)
plt.suptitle("Regression of Numeric Features", size=20, weight='bold')
j = 0
for i in columns_list:
    ax = plt.subplot(331 + j)
    ax = sns.regplot(data=df, x=i, y='price', line_kws={'color':'#ff8c00'})
    ax.set_title("Price and {} comparision analysis".format(i))    
    j = j + 1
    for s in ['left','right','top','bottom']:
        ax.spines[s].set_visible(False)
plt.show()
```

<img src="../assets/images/result5.png" width="100%">

결과 그래프는 Price와의 선형 관계를 보여주지만 Outlier가 존재한다.

```python
fig = plt.figure(figsize=(15, 10), constrained_layout=True)
plt.suptitle("Box Plot for Numeric Features", size=20, weight='bold')
j = 0
for i in columns_list:
    ax = plt.subplot(331 + j)
    ax = sns.boxplot(data=df, x=i)
    ax.set_title("Box plot for {}".format(i))
    for s in ['left','right','top','bottom']:
        ax.spines[s].set_visible(False)
    j = j + 1
plt.show()
```

<img src="../assets/images/result6.png" width="100%">

Price에 대한 BoxPlot을 통해 구체적인 Outlier을 확인할 수 있었다. 보다 정확한 분석을 위해 Outlier을 제거해야한다.

```python
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df_clean = df[~((df < (Q1 - 1.5*IQR)) | (df > (Q3 + 1.5*IQR))).any(axis=1)]

columns_list = df_clean.select_dtypes(exclude='object').columns.to_list()
columns_list.remove('price')

fig = plt.figure(figsize=(15, 10))
plt.suptitle("Box plot for Numeric features after Outlier removal", size=20, weight='bold')
j = 0
for i in columns_list:
    ax = plt.subplot(331 + j)
    ax = sns.boxplot(data=df_clean, x=i)
    ax.set_title("Box plot for {}".format(i))
    for s in ['left', 'right', 'top', 'bottom']:
        ax.spines[s].set_visible(False)
    j = j + 1
plt.show()
```

<img src="../assets/images/result8.png" width="100%">

IQR Rule에 따라 Q3 + (1.5 * IQR) 이상, Q1 – (1.5 * IQR) 이하의 값을 Outlier로 정의하고 이를 제거하였다. BoxPlot을 통해 Outlier가 성공적으로 제거되었음을 확인할 수 있었다.

### 3.3. Feature Encoding
문자열 값을 입력 값으로 받을 수 없기 때문에 모든 문자열 값들을 숫자형으로 인코딩하는 Preprocessing(전처리) 작업 후에 ML Model에 학습을 시켜야 한다.

```python
category_columns = df.select_dtypes(include='object').columns.to_list()
df_onehot = pd.get_dummies(df_clean, columns=category_columns, drop_first=True)
df_onehot.head()
```

<img src="../assets/images/result9.png" width="100%">

가공 상태, 색, 투명도에 대해 One-hot Encoding을 진행하여 문자열 데이터를 0과 1로 변환하였다.

### 3.4. Train & Test Split
주어진 데이터셋에 대해 Train에 사용할 데이터와 Test에 사용할 데이터의 분류가 필요하다.

```python
from sklearn.model_selection import train_test_split

X = df_onehot.drop('price', axis=1)
y = df_onehot['price']
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=123)
```

Train 과 Test 데이터의 비율은 8:2로 설정하여 분류를 진행하였다.

## 4. Model Create & Evaluation
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
prediction = model.predict(X_test)
```

Input값에 대해 발생하는 Output 예측하는 가장 직관적인 모델 중 하나인 Linear Regression을 사용하였다.

```python
from sklearn.metrics import mean_squared_error

fig = plt.figure(figsize=(15,10))
residual = y_test - prediction
plt.suptitle("Comparing y_test and Predicted value", size=20, weight='bold')
ax = fig.subplot_mosaic("""AA
                           BB
                           CC""")
sns.scatterplot(y_test, residual, ax=ax['A'])
ax['A'].axhline(y=0, ls='--', linewidth=3)
sns.kdeplot(residual, ax=ax['B'], fill=True, linewidth=2)

ax['C'].text(x=0.2, y=0.2, s="MSE(Mean Squared Error): {}".format(np.round(mean_squared_error(y_test, prediction, squared=False),2)), ha='left', weight='bold', size=14)
ax['C'].text(x=0.2, y=0.4, s="Accuracy of model with Train data: {}".format(np.round(model.score(X_train, y_train),2)), ha='left', weight='bold', size=14)
ax['C'].text(x=0.2, y=0.6, s="Accuracy of model with Test data: {}".format(np.round(model.score(X_test, y_test),2)), ha='left', weight='bold', size=14)
ax['C'].text(x=0.2, y=0.8, s="Result:", ha='left', weight='bold', size=14)

ax['C'].axis('off')

for i in 'ABC':
    for s in ['left', 'right', 'top', 'bottom']:
        ax[i].spines[s].set_visible(False)
plt.show()
```

<img src="../assets/images/result10.png" width="100%">

Test Data에 대한 모델의 정확도는 0.93, Train Data에 대한 모델의 정확도는 0.93이 나왔으며 MSE(Mean Squared Error)은 743.45가 나왔다.


## 5. Conclusion


## 6. Reference
